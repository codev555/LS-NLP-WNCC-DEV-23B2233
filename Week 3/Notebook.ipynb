{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# sentiment_pipeline.py\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorWithPadding\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\n\n# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# 1. Load the IMDb dataset\ndataset = load_dataset(\"imdb\")\n\n# 2. Load tokenizer and model\nmodel_name = \"bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n\n# 3. Tokenization function\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], truncation=True, padding=True)\n\n# 4. Preprocess dataset\nencoded_dataset = dataset.map(tokenize, batched=True)\nencoded_dataset = encoded_dataset.remove_columns([\"text\"])\nencoded_dataset.set_format(\"torch\")\n\n# 5. Evaluation metrics\ndef compute_metrics(pred):\n    logits, labels = pred\n    predictions = np.argmax(logits, axis=-1)\n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions)\n    return {\"accuracy\": acc, \"f1\": f1}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T22:43:47.628427Z","iopub.execute_input":"2025-06-27T22:43:47.628659Z","iopub.status.idle":"2025-06-27T22:50:43.100864Z","shell.execute_reply.started":"2025-06-27T22:43:47.628635Z","shell.execute_reply":"2025-06-27T22:50:43.100336Z"}},"outputs":[{"name":"stderr","text":"2025-06-27 22:44:01.867689: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751064242.049013      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751064242.105081      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e980fde666b461abfc15259b7f40ff2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a81328bd3aac4223aa94cc1ae5e9066a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"217972e1f7dc470c8bbf11d6ed29ce30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaf18a652d6f4f37906b2b196d3702de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"538a5356714541c784535071cb33f532"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f75411895f044cc3984295550f451072"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be0bd856cd5c4212a4a9da41aaeb16aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61850f4974e6493fbb85272c706101e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5870d0e60a94e638268833c841e246e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceddb2817ddd4787bf037c1365920087"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e026745d116c4a12a617ea1fa8a648b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8768b81755854d42a6c66364c99c56a1"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"050ec969d2d3407a8376798dceb43181"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"088e74bddd374ba4909abae724ec5481"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4be520558f3943f8aa595c1eb73e13a1"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer, DataCollatorWithPadding, logging\n\n# Optional: enable informative logging for progress display\nlogging.set_verbosity_info()\n\n# 6. TrainingArguments (with logging and tqdm settings)\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    run_name=\"sentiment_analysis_run\",     # avoids wandb warning\n    eval_strategy=\"epoch\",                 # for evaluation each epoch\n    save_strategy=\"epoch\",                 # save checkpoint each epoch\n    num_train_epochs=2,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_dir=\"./logs\",\n    logging_steps=10,                      # log every 10 steps\n    logging_first_step=True,               # also log at step 1\n    report_to=\"none\",                      # disable wandb/tensorboard\n    disable_tqdm=False,                    # enable progress bar\n)\n\n# 7. Trainer setup (no tokenizer= to avoid deprecation warning)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset[\"train\"].shuffle(seed=42).select(range(2000)),\n    eval_dataset=encoded_dataset[\"test\"].select(range(500)),\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# 8. Train the model\ntrainer.train()\n\n# 9. Save model\nmodel.save_pretrained(\"sentiment_model\")\ntokenizer.save_pretrained(\"sentiment_model\")\n\n# 10. Load and test on sample\ndef predict(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    prediction = torch.argmax(logits, dim=-1).item()\n    return \"positive\" if prediction == 1 else \"negative\"\n\n# Sample prediction\nprint(\"Sample prediction:\", predict(\"The movie was absolutely amazing!\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T22:58:59.382068Z","iopub.execute_input":"2025-06-27T22:58:59.382792Z","iopub.status.idle":"2025-06-27T23:04:10.373444Z","shell.execute_reply.started":"2025-06-27T22:58:59.382768Z","shell.execute_reply":"2025-06-27T23:04:10.372723Z"}},"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n***** Running training *****\n  Num examples = 2,000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 500\n  Number of trainable parameters = 109,483,778\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 05:08, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.534100</td>\n      <td>0.334949</td>\n      <td>0.900000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.257700</td>\n      <td>0.387983</td>\n      <td>0.906000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 500\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-250\nConfiguration saved in ./results/checkpoint-250/config.json\nModel weights saved in ./results/checkpoint-250/model.safetensors\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\ntokenizer config file saved in ./results/checkpoint-250/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-250/special_tokens_map.json\n\n***** Running Evaluation *****\n  Num examples = 500\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-500\nConfiguration saved in ./results/checkpoint-500/config.json\nModel weights saved in ./results/checkpoint-500/model.safetensors\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\ntokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nConfiguration saved in sentiment_model/config.json\nModel weights saved in sentiment_model/model.safetensors\ntokenizer config file saved in sentiment_model/tokenizer_config.json\nSpecial tokens file saved in sentiment_model/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"Sample prediction: positive\n","output_type":"stream"}],"execution_count":4}]}