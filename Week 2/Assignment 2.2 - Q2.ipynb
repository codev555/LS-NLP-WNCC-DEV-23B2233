{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80c28aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  @AmericanAir In car gng to DFW. Pulled over 1h...   \n",
      "1  @AmericanAir after all, the plane didn’t land ...   \n",
      "2  @SouthwestAir can't believe how many paying cu...   \n",
      "3  @USAirways I can legitimately say that I would...   \n",
      "4  @AmericanAir still no response from AA. great ...   \n",
      "\n",
      "                                              Tokens  \n",
      "0  [car, gng, dfw, pulled, hr, ago, icy, road, on...  \n",
      "1  [plane, land, identical, worse, condition, grk...  \n",
      "2  [believe, many, paying, customer, left, high, ...  \n",
      "3  [legitimately, say, would, rather, driven, cro...  \n",
      "4             [still, response, aa, great, job, guy]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download once\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"test_twitter_x_test.csv\")[['text']]\n",
    "df.columns = ['Text']\n",
    "df = df.dropna()\n",
    "\n",
    "# Initialize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_tweet(text):\n",
    "    text = contractions.fix(text.lower())\n",
    "    text = re.sub(r'http\\S+|@\\w+|#\\w+|[^a-zA-Z\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and t not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "df['Tokens'] = df['Text'].apply(preprocess_tweet)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa8488",
   "metadata": {},
   "source": [
    "#### (Python 3.11 Environment)\n",
    "\n",
    "### Step 1: Preprocessing Tweets\n",
    "\n",
    "We begin by preprocessing each tweet using the following steps:\n",
    "- Convert to lowercase\n",
    "- Expand contractions (e.g., \"can't\" → \"cannot\")\n",
    "- Remove URLs, mentions, hashtags, punctuation\n",
    "- Tokenize into words using `nltk.word_tokenize()`\n",
    "- Remove stopwords and lemmatize tokens\n",
    "\n",
    "This transforms raw tweets into clean word lists for further vectorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be815240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model loaded. Vector size: 300\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load Google News Word2Vec binary model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(\n",
    "    \"GoogleNews-vectors-negative300.bin.gz\", binary=True\n",
    ")\n",
    "\n",
    "# Confirm dimensions\n",
    "print(f\"Word2Vec model loaded. Vector size: {word2vec_model.vector_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efec3aa",
   "metadata": {},
   "source": [
    "### Step 2: Load Google News Word2Vec Model using Gensim\n",
    "\n",
    "We load the pretrained **Google News Word2Vec** model using `gensim.models.KeyedVectors`. This model contains 3 million English word vectors, each of 300 dimensions.\n",
    "\n",
    "The model is loaded from the binary `.bin.gz` file using:\n",
    "\n",
    "```python\n",
    "'KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)'\n",
    "```\n",
    "\n",
    "This provides vector representations for individual words, which we will use to build tweet-level embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c317b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  @AmericanAir In car gng to DFW. Pulled over 1h...   \n",
      "1  @AmericanAir after all, the plane didn’t land ...   \n",
      "2  @SouthwestAir can't believe how many paying cu...   \n",
      "3  @USAirways I can legitimately say that I would...   \n",
      "4  @AmericanAir still no response from AA. great ...   \n",
      "\n",
      "                                              Vector  \n",
      "0  [-0.323791, 0.52586997, 0.02905167, 0.24832352...  \n",
      "1  [0.6987391, -0.55277914, 0.060138952, -0.59743...  \n",
      "2  [0.38663167, 0.36437044, -0.09021091, 0.616413...  \n",
      "3  [-0.042288974, 0.36418095, 0.36169785, 0.55547...  \n",
      "4  [0.50087684, 0.32249323, -0.0906745, 0.2391912...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Create TF-IDF dictionary from CleanText (tokens joined into strings)\n",
    "df['CleanText'] = df['Tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(df['CleanText'])\n",
    "tfidf_dict = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
    "\n",
    "# Weighted average vector using Word2Vec + TF-IDF\n",
    "def tfidf_weighted_word2vec(tokens, model, tfidf_dict, vector_size):\n",
    "    vectors = [\n",
    "        model[word] * tfidf_dict[word]\n",
    "        for word in tokens if word in model and word in tfidf_dict\n",
    "    ]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "# Compute final vector for each tweet\n",
    "df['Vector'] = df['Tokens'].apply(lambda tokens: tfidf_weighted_word2vec(tokens, word2vec_model, tfidf_dict, word2vec_model.vector_size))\n",
    "\n",
    "# View result\n",
    "print(df[['Text', 'Vector']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db28dcb",
   "metadata": {},
   "source": [
    "### Step 3: Convert Tweets to Fixed-Length Vectors Using Word2Vec + TF-IDF\n",
    "\n",
    "We generate a 300-dimensional vector representation for each tweet by combining:\n",
    "\n",
    "1. **Pretrained Google Word2Vec model**: provides word-level 300D embeddings.\n",
    "2. **TF-IDF weighting**: measures word importance within the dataset.\n",
    "\n",
    "For each tweet:\n",
    "- We extract tokens.\n",
    "- For each token in the Word2Vec vocabulary and TF-IDF dict:\n",
    "  - Multiply its embedding by its TF-IDF weight.\n",
    "- Average all such weighted embeddings to get a single vector.\n",
    "\n",
    "This produces one semantically rich vector per tweet, ready for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46561565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.40      0.52       545\n",
      "     neutral       0.16      0.31      0.21       108\n",
      "    positive       0.11      0.32      0.17        79\n",
      "\n",
      "    accuracy                           0.38       732\n",
      "   macro avg       0.33      0.34      0.30       732\n",
      "weighted avg       0.57      0.38      0.43       732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Add sentiment column from original file\n",
    "df_sent = pd.read_csv(\"test_twitter_x_test.csv\")[['sentiment']]\n",
    "df['Sentiment'] = df_sent['sentiment']\n",
    "\n",
    "# Encode labels to 0,1,2\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Sentiment'])\n",
    "X = np.vstack(df['Vector'].values)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train logistic regression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46063339",
   "metadata": {},
   "source": [
    "### Step 4: Train a Classifier Using Tweet Vectors\n",
    "\n",
    "We train a sentiment classifier using the tweet embeddings created in Step 3. ('38% acc' obtained)\n",
    "\n",
    "Steps:\n",
    "1. Load the `sentiment` labels and encode them to numeric format (0=negative, 1=neutral, 2=positive).\n",
    "2. Split the dataset into train and test sets using `train_test_split`.\n",
    "3. Use `SMOTE` to balance class distribution in the training set.\n",
    "4. Train a `LogisticRegression` model on the resampled vectors.\n",
    "5. Evaluate the model using precision, recall, and F1-score via `classification_report`.\n",
    "\n",
    "This trained model will be used to classify unseen tweets in later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd11e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe model loaded. Vector size: 100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.35      0.47       545\n",
      "     neutral       0.14      0.31      0.19       108\n",
      "    positive       0.10      0.28      0.14        79\n",
      "\n",
      "    accuracy                           0.34       732\n",
      "   macro avg       0.32      0.31      0.27       732\n",
      "weighted avg       0.56      0.34      0.40       732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load GloVe 100D\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "print(f\"GloVe model loaded. Vector size: {glove_model.vector_size}\")\n",
    "\n",
    "# Reload DataFrame with Sentiment\n",
    "df = pd.read_csv(\"test_twitter_x_test.csv\")[['text', 'sentiment']]\n",
    "df.columns = ['Text', 'Sentiment']\n",
    "df.dropna(subset=['Text'], inplace=True)\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "\n",
    "# Preprocess\n",
    "df['Tokens'] = df['Text'].apply(preprocess_tweet)\n",
    "df['CleanText'] = df['Tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# TF-IDF for GloVe\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(df['CleanText'])\n",
    "tfidf_dict = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
    "\n",
    "# GloVe vectorization using TF-IDF\n",
    "def tfidf_weighted_glove(tokens, model, tfidf_dict, vector_size):\n",
    "    vectors = [model[word] * tfidf_dict[word] for word in tokens if word in model and word in tfidf_dict]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "# Generate tweet vectors\n",
    "df['Vector'] = df['Tokens'].apply(lambda tokens: tfidf_weighted_glove(tokens, glove_model, tfidf_dict, glove_model.vector_size))\n",
    "X = np.vstack(df['Vector'].values)\n",
    "y = LabelEncoder().fit_transform(df['Sentiment'])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Balance with SMOTE\n",
    "X_train_bal, y_train_bal = SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
    "\n",
    "# Train on GloVe\n",
    "glove_model_classifier = LogisticRegression(max_iter=1000)\n",
    "glove_model_classifier.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = glove_model_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb40684",
   "metadata": {},
   "source": [
    "### Step 5: Train Classifier Using GloVe 100D Vectors \n",
    "\n",
    "To resolve dimension mismatch issues in prediction, we train a second classifier using **GloVe 100D vectors**. ('34% acc' obtained)\n",
    "\n",
    "Steps:\n",
    "1. Load the `glove-wiki-gigaword-100` embedding model.\n",
    "2. Preprocess tweets and generate TF-IDF dictionary.\n",
    "3. Vectorize each tweet using a **TF-IDF weighted average of GloVe vectors**.\n",
    "4. Split the dataset and balance classes using `SMOTE`.\n",
    "5. Train a `LogisticRegression` model on the 100D tweet vectors.\n",
    "\n",
    "This GloVe-trained model will be used in the next step to predict sentiment for new tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2af267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe-based TF-IDF weighted average vector\n",
    "def tfidf_weighted_glove(tokens, glove_model, tfidf_dict, vector_size):\n",
    "    vectors = [\n",
    "        glove_model[word] * tfidf_dict[word]\n",
    "        for word in tokens if word in glove_model and word in tfidf_dict\n",
    "    ]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "# Prediction function\n",
    "def predict_tweet_sentiment(tweet, model, glove_model, tfidf_dict):\n",
    "    tokens = preprocess_tweet(tweet)\n",
    "    vector = tfidf_weighted_glove(tokens, glove_model, tfidf_dict, glove_model.vector_size).reshape(1, -1)\n",
    "    pred = model.predict(vector)[0]\n",
    "    return pred  # 0 = negative, 1 = neutral, 2 = positive \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35727f2",
   "metadata": {},
   "source": [
    "### Step 6: Predict Sentiment for a New Tweet Using GloVe-Based Model\n",
    "\n",
    "In this final step, we use the model trained on GloVe 100D vectors to classify the sentiment of any input tweet.\n",
    "\n",
    "Steps:\n",
    "1. Preprocess the tweet (cleaning, tokenization, lemmatization)\n",
    "2. Create a TF-IDF weighted average of GloVe embeddings for the tokens\n",
    "3. Use the trained model to predict sentiment\n",
    "\n",
    "This function enables real-time sentiment analysis on unseen tweets.\n",
    "\n",
    "Label mapping:\n",
    "- `0` → Negative\n",
    "- `1` → Neutral\n",
    "- `2` → Positive\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
