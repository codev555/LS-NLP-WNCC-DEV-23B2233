{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2fc5043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Message  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                              Tokens  \n",
      "0  [go, jurong, point, crazy, available, bugis, n...  \n",
      "1                     [ok, lar, joking, wif, u, oni]  \n",
      "2  [free, entry, wkly, comp, win, fa, cup, final,...  \n",
      "3      [u, dun, say, early, hor, u, c, already, say]  \n",
      "4     [nah, think, goes, usf, lives, around, though]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')[['v1', 'v2']]\n",
    "df.columns = ['Label', 'Message']\n",
    "\n",
    "# Preprocessing: lowercase, tokenize, remove stopwords, keep alpha only\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "# Apply preprocessing\n",
    "df['Tokens'] = df['Message'].apply(preprocess_text)\n",
    "\n",
    "# Preview result\n",
    "print(df[['Message', 'Tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd1bf32",
   "metadata": {},
   "source": [
    "### Step 1: Preprocessing Messages\n",
    "\n",
    "We begin by loading the dataset and preprocessing each SMS message. This includes:\n",
    "- Lowercasing the text\n",
    "- Tokenizing using NLTK\n",
    "- Removing English stopwords\n",
    "- Filtering out non-alphabetic tokens\n",
    "\n",
    "The cleaned token list for each message is stored in a new `Tokens` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b654570b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Vector size: 300\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the Google News Word2Vec model from local binary file\n",
    "w2v_model = KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin.gz',\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "# Confirm model details\n",
    "print(f\"Model loaded. Vector size: {w2v_model.vector_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c641a8",
   "metadata": {},
   "source": [
    "### (Python 3.11 Environment)\n",
    "### Step 2: Load Pre-trained Word2Vec Model\n",
    "\n",
    "We load the 300-dimensional Word2Vec model trained on Google News using `gensim`'s `KeyedVectors.load_word2vec_format`. This model contains over 3 million word and phrase vectors and allows us to convert tokens into dense embeddings.\n",
    "\n",
    "Make sure the `.bin.gz` file is present locally in your working directory. Once loaded, we can access each wordâ€™s corresponding vector using `w2v_model[word]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22da1713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Message  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                              Vector  \n",
      "0  [-0.019805908, 0.05167062, 0.02709961, 0.21868...  \n",
      "1  [-0.06323496, 0.0803833, 0.060943604, 0.102498...  \n",
      "2  [-0.03242302, -0.0050720214, -0.06273012, 0.11...  \n",
      "3  [-0.06568061, 0.0262146, 0.1081543, 0.0869751,...  \n",
      "4  [0.032470703, 0.037462506, 0.047345843, 0.1572...  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function: Average vectors of valid tokens\n",
    "def get_average_vector(tokens, model, vector_size):\n",
    "    vectors = [model[token] for token in tokens if token in model]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Vector size from model\n",
    "vector_size = w2v_model.vector_size\n",
    "\n",
    "# Apply to each message\n",
    "df['Vector'] = df['Tokens'].apply(lambda tokens: get_average_vector(tokens, w2v_model, vector_size))\n",
    "\n",
    "# Show a sample\n",
    "print(df[['Message', 'Vector']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991608a3",
   "metadata": {},
   "source": [
    "### Step 3: Convert Messages to Word2Vec Vectors\n",
    "\n",
    "We represent each message as the average of its Word2Vec token embeddings. For each token present in the model, we fetch its vector. If no token is found, a zero vector is used. This results in a uniform 300-dimensional vector per message, ready for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71c4a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      0.98      0.97       965\n",
      "        spam       0.82      0.72      0.77       150\n",
      "\n",
      "    accuracy                           0.94      1115\n",
      "   macro avg       0.89      0.85      0.87      1115\n",
      "weighted avg       0.94      0.94      0.94      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Convert labels to binary\n",
    "le = LabelEncoder()\n",
    "df['LabelEncoded'] = le.fit_transform(df['Label'])  # ham = 0, spam = 1\n",
    "\n",
    "# Prepare feature matrix and labels\n",
    "X = np.vstack(df['Vector'].values)\n",
    "y = df['LabelEncoded'].values\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2ac82",
   "metadata": {},
   "source": [
    "### Step 4: Classification with Logistic Regression\n",
    "\n",
    "We encode labels, split the data, train a logistic regression model, and evaluate it using a classification report. This gives us metrics like precision, recall, and F1-score for both spam and ham messages.\n",
    "\n",
    "#### Classification Report\n",
    "\n",
    "The logistic regression classifier achieved 94% overall accuracy. While it performs well on ham messages, the recall for spam messages (72%) could be improved with more advanced classifiers or oversampling techniques. Still, this demonstrates a strong baseline for spam detection using Word2Vec embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e3bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_message_class(msg):\n",
    "    # Preprocess the message\n",
    "    tokens = preprocess_text(msg)  # use same preprocessing as before\n",
    "\n",
    "    # Convert to vector\n",
    "    vector = get_average_vector(tokens, w2v_model, w2v_model.vector_size).reshape(1, -1)\n",
    "\n",
    "    # Predict\n",
    "    prediction = clf.predict(vector)[0]\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8063dc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(predict_message_class(\"Congratulations! You have won a free ticket to Bahamas.\"))  # likely 1 (spam)\n",
    "print(predict_message_class(\"Can we meet tomorrow at 10?\"))  # likely 0 (ham)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11150a93",
   "metadata": {},
   "source": [
    "### Step 5: Predict Message Class\n",
    "\n",
    "We define `predict_message_class(msg)` which accepts a raw SMS message string. It applies the same preprocessing and vectorization steps used during training, then uses the trained logistic regression classifier to return:\n",
    "- `0` for ham\n",
    "- `1` for spam\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
