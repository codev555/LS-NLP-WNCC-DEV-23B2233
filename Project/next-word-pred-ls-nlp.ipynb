{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets evaluate --no-deps\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T23:59:16.277890Z","iopub.execute_input":"2025-07-05T23:59:16.278056Z","iopub.status.idle":"2025-07-05T23:59:19.114198Z","shell.execute_reply.started":"2025-07-05T23:59:16.278040Z","shell.execute_reply":"2025-07-05T23:59:19.113465Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Step 1: Installing Required Libraries (Safely for Kaggle)\n\nWe're installing:\n- `transformers`: For the GPT-2 model and tokenizer.\n- `datasets`: To load datasets like WikiText-2.\n- `evaluate`: To compute evaluation metrics like perplexity.\n\nWe use `--no-deps` to prevent Kaggle’s preinstalled environment from throwing conflicts. These libraries usually work well with the already installed dependencies on Kaggle.\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nimport evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T23:59:22.163739Z","iopub.execute_input":"2025-07-05T23:59:22.164301Z","iopub.status.idle":"2025-07-05T23:59:55.882045Z","shell.execute_reply.started":"2025-07-05T23:59:22.164270Z","shell.execute_reply":"2025-07-05T23:59:55.881474Z"}},"outputs":[{"name":"stderr","text":"2025-07-05 23:59:42.492096: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751759982.696770      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751759982.761491      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Step 2: Importing Libraries\n\nWe now import the core libraries:\n- `torch`: For GPU-accelerated training.\n- `load_dataset`: To load text datasets from Hugging Face.\n- `GPT2Tokenizer` and `GPT2LMHeadModel`: To load the tokenizer and model for GPT-2.\n- `DataCollatorForLanguageModeling`: Automatically handles masking and padding during training.\n- `Trainer` and `TrainingArguments`: Simplifies training and evaluation.\n- `evaluate`: Used for calculating perplexity and accuracy.\n","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\ndataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T00:00:00.568238Z","iopub.execute_input":"2025-07-06T00:00:00.568844Z","iopub.status.idle":"2025-07-06T00:00:05.606300Z","shell.execute_reply.started":"2025-07-06T00:00:00.568821Z","shell.execute_reply":"2025-07-06T00:00:05.605761Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"107990796dfc41bf82073f5f8fc8c5fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afe6edf1e8c142ff924c332c13da1bdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfbcc028d1bf4967861dfe30c2a59f73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5314b3f6f1824e46bf4e27eb2dd1221d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51ab7018e0f54475878bf3b5771aca8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"377f01c0f9ca444eb164a5ac611412c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e776c70e6541318967d46756134c7c"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 36718\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 3760\n    })\n})"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## Step 3: Loading Dataset\n\nWe're using the **WikiText-2** dataset from Hugging Face’s `datasets` library. It’s a collection of clean Wikipedia articles, ideal for language modeling tasks like next-word prediction.\n\nWe load the \"wikitext-2-raw-v1\" version, which keeps the raw text intact (without preprocessing).\n\nThe dataset will include `train`, `validation`, and `test` splits.\n","metadata":{}},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token  # GPT-2 does not have a pad token; setting it to eos\n\ndef tokenize_function(example):\n    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T00:00:11.106111Z","iopub.execute_input":"2025-07-06T00:00:11.106765Z","iopub.status.idle":"2025-07-06T00:00:38.680554Z","shell.execute_reply.started":"2025-07-06T00:00:11.106741Z","shell.execute_reply":"2025-07-06T00:00:38.679912Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18b2537056d443989e8a7909b18f39d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f85ca8c307d44d26b31882b89d0ec824"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"016ee27226704ad4911cd5c9bf97a909"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea33fe4a9336443595831b5483de471a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60286732bcf34b429e72b6f2d60cfc0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4609248ef364ebca890027529c1dce4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fa662825bdf40d3b86bbe30d0dc3485"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1294ba90a6344ec87010265212cb9d6"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## Step 4: Tokenizing the Text\n\nWe load the GPT-2 tokenizer using Hugging Face’s `AutoTokenizer`. GPT-2 doesn't have a native padding token, so we set `pad_token` to the end-of-sequence token (`eos_token`).\n\nWe then define a tokenization function:\n- It tokenizes each line of text.\n- It truncates sequences to a fixed length (`max_length=128`).\n- It pads shorter sequences to match the max length.\n\nUsing `dataset.map()`, we apply this function to all examples in the dataset. We also remove the original `'text'` column to keep only tokenized data.\n","metadata":{}},{"cell_type":"code","source":"model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\nmodel.resize_token_embeddings(len(tokenizer)) \n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T00:00:45.050114Z","iopub.execute_input":"2025-07-06T00:00:45.050367Z","iopub.status.idle":"2025-07-06T00:00:48.175203Z","shell.execute_reply.started":"2025-07-06T00:00:45.050350Z","shell.execute_reply":"2025-07-06T00:00:48.174442Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0646c273dc384c3c96f1f7f37628747b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d2719e2c64047e5bd96f367e1b2eec3"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## Step 5: Loading GPT-2 Model and Data Collator\n\nWe load the **GPT-2 language model** with `GPT2LMHeadModel`. This version is specifically designed for **causal language modeling** — predicting the next word.\n\nSince we added a padding token, we resize the token embeddings to align with the tokenizer vocabulary size.\n\nWe also set up a `DataCollatorForLanguageModeling`:\n- It dynamically pads inputs during training.\n- We set `mlm=False` because we're doing **causal** (next-word) prediction, not masked language modeling like BERT.\n","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./gpt2-nextword\",\n    eval_strategy=\"epoch\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=2,  \n    weight_decay=0.01,\n    save_strategy=\"no\",\n    logging_steps=100,\n    push_to_hub=False,\n    report_to=\"none\",\n    fp16=torch.cuda.is_available(),  \n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T00:00:58.834671Z","iopub.execute_input":"2025-07-06T00:00:58.835391Z","iopub.status.idle":"2025-07-06T00:34:57.029260Z","shell.execute_reply.started":"2025-07-06T00:00:58.835362Z","shell.execute_reply":"2025-07-06T00:34:57.028547Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/4252057619.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18360' max='18360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [18360/18360 33:56, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.160300</td>\n      <td>3.339974</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.842200</td>\n      <td>3.323588</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=18360, training_loss=3.126224422247062, metrics={'train_runtime': 2037.4134, 'train_samples_per_second': 36.044, 'train_steps_per_second': 9.011, 'total_flos': 4797060415488000.0, 'train_loss': 3.126224422247062, 'epoch': 2.0})"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Step 6: Fine-Tuning GPT-2 using Hugging Face Trainer\n\nWe set up `TrainingArguments` to define the training configuration :\n- `per_device_train_batch_size=4`: Keeps memory use low on Kaggle.\n- `num_train_epochs=2`: 2 epochs for a quicker fit. \n- `eval_strategy=\"epoch\"`: Evaluates after each epoch.\n- `fp16`: Enables mixed-precision training if GPU is available for faster performance.\n\nWe use the Hugging Face `Trainer` to :\n- Train the GPT-2 model on the tokenized `train` dataset.\n- Evaluate it on the `validation` split.\n- Handle batching and padding using the data collator.\n","metadata":{}},{"cell_type":"code","source":"import math\n\n# Perplexity\neval_results = trainer.evaluate()\nperplexity = math.exp(eval_results[\"eval_loss\"])\nprint(f\"Perplexity: {perplexity:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T00:41:08.239028Z","iopub.execute_input":"2025-07-06T00:41:08.239311Z","iopub.status.idle":"2025-07-06T00:41:34.723430Z","shell.execute_reply.started":"2025-07-06T00:41:08.239290Z","shell.execute_reply":"2025-07-06T00:41:34.722857Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='940' max='940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [940/940 00:26]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Perplexity: 27.76\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Step 7: Evaluation – Perplexity\n\n**Perplexity** is a key metric for language modeling — lower values mean better predictions.\n\nWe use the evaluation loss returned by `trainer.evaluate()` and apply `math.exp()` to convert it into perplexity:\n\n$$\n\\text{Perplexity} = e^{\\text{Loss}}\n$$\n\nThis tells us how confidently the model predicts the next word — the closer the value is to 1, the better the predictions.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom torch.nn.functional import softmax\n\ndef compute_top_k_accuracy_fixed(model, tokenizer, dataset, k=5, num_samples=100):\n    model.eval()\n    correct = 0\n    total = 0\n\n    for i in range(min(num_samples, len(dataset))):\n        inputs = torch.tensor(dataset[i][\"input_ids\"]).unsqueeze(0).to(model.device)\n\n        # Skip if input is all padding\n        if torch.all(inputs == tokenizer.pad_token_id):\n            continue\n\n        with torch.no_grad():\n            outputs = model(inputs)\n            logits = outputs.logits\n\n        # Shift logits and labels so that the prediction for token t is compared to token t+1\n        shift_logits = logits[:, :-1, :]\n        shift_labels = inputs[:, 1:]\n\n        # Getting top-k predictions\n        top_k_preds = torch.topk(shift_logits, k, dim=-1).indices\n\n        # Checking if actual next token is in top-k predictions\n        for pos in range(shift_labels.shape[1]):\n            label = shift_labels[0, pos]\n            if label != tokenizer.pad_token_id:\n                if label in top_k_preds[0, pos]:\n                    correct += 1\n                total += 1\n\n    accuracy = correct / total if total > 0 else 0\n    print(f\"Top-{k} Accuracy : {accuracy * 100:.2f}%\")\n\ncompute_top_k_accuracy_fixed(model, tokenizer, tokenized_datasets[\"validation\"], k=5, num_samples=100)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T00:41:43.894062Z","iopub.execute_input":"2025-07-06T00:41:43.894788Z","iopub.status.idle":"2025-07-06T00:41:45.400731Z","shell.execute_reply.started":"2025-07-06T00:41:43.894760Z","shell.execute_reply":"2025-07-06T00:41:45.400046Z"}},"outputs":[{"name":"stdout","text":"Top-5 Accuracy : 62.82%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Step 7: Evaluation – Top-5 Accuracy (Improved Method)\n\nOur initial implementation of Top-k accuracy returned 0%, likely due to comparing padded or out-of-bounds tokens.\n\nWe now use a **more robust method**:\n- The model predicts the next token for each position in the sequence.\n- We shift the input and labels to align predictions at time `t` with actual tokens at `t+1`.\n- We compute whether the **actual next token is in the top-5 predictions** at each step.\n- Padding tokens are ignored.\n\nThis yields a **Top-5 Accuracy of 62.82%**, showing the model correctly includes the true next word in its top 5 predictions **over 60% of the time** — strong performance for a lightweight, fine-tuned GPT-2.\n","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"./gpt2-nextword-model\")\ntokenizer.save_pretrained(\"./gpt2-nextword-model\")\n\n!zip -r /kaggle/working/gpt2-nextword-model.zip ./gpt2-nextword-model > /dev/null\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T00:45:37.486878Z","iopub.execute_input":"2025-07-06T00:45:37.487163Z","iopub.status.idle":"2025-07-06T00:46:04.092723Z","shell.execute_reply.started":"2025-07-06T00:45:37.487142Z","shell.execute_reply":"2025-07-06T00:46:04.091831Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Step 8: Saving the Fine-Tuned Model for Download\n\nTo preserve and download the trained model :\n- We save the fine-tuned GPT-2 model and tokenizer using `save_pretrained()`.\n- We zip the entire folder and move it to the `/kaggle/working/` directory, which is exposed for downloading in Kaggle.\n","metadata":{}},{"cell_type":"markdown","source":"# Final Project Summary – Next Word Predictor using Transformers\n\n## Objective\nTo build and evaluate a next-word predictor using a fine-tuned GPT-2 transformer model. This foundational NLP task enables autocomplete, chatbots, and intelligent writing assistants.\n\n## Model & Dataset\n- **Model**: GPT-2 (`GPT2LMHeadModel`) from Hugging Face.\n- **Tokenizer**: GPT-2 tokenizer with padding aligned to EOS token.\n- **Dataset**: `wikitext-2-raw-v1` – clean Wikipedia corpus, loaded via Hugging Face `datasets`.\n\n## Training Details\n- Fine-tuned GPT-2 using the `Trainer` API.\n- Epochs: 2\n- Batch Size: 4\n- Padding: Dynamic (via `DataCollatorForLanguageModeling`)\n- Loss: Causal Language Modeling (next-token prediction)\n\n## Evaluation Results\n- **Perplexity**: 27.76  \n  Indicates decent language fluency and predictability.\n- **Top-5 Accuracy**: 62.82%  \n  The correct next token appears in the top-5 predictions ~63% of the time.\n\n## Key Learnings\n- Fine-tuning pretrained models like GPT-2 can be efficient even with small corpora.\n- Perplexity gives insight into how \"surprised\" the model is by the true next word.\n- Top-k accuracy reflects practical autocomplete performance.\n\n## Conclusion\nThe project demonstrates a working pipeline for next-word prediction using a transformer model. Results show strong potential for real-world applications like typing assistants, writing tools, or chat interfaces.\n","metadata":{}}]}