Problem Statement :
Design and train a transformer-based language model to predict the next word in a given text sequence. This task is foundational in NLP and supports applications such as autocomplete, text generation, and intelligent writing assistants.

Objectives :
Build a language model for next-word prediction using transformer architecture.
Fine-tune a pre-trained model (e.g., GPT-2) on a textual dataset.
Evaluate the model using standard metrics like perplexity and top-k accuracy.
Understand and apply best practices for tokenizer alignment, model adaptation, and text preprocessing.
